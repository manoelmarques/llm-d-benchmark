## llm-d-inference-sim for GPU-less clusters
# NOTE: This scenario is not complete on its own
#       It must be appended to another scenario

export LLMDBENCH_LLMD_IMAGE_NAME="llm-d-inference-sim"

export LLMDBENCH_VLLM_COMMON_AFFINITY=kubernetes.io/os:linux
export LLMDBENCH_VLLM_COMMON_ACCELERATOR_NR="0"
export LLMDBENCH_VLLM_COMMON_CPU_NR="0"
export LLMDBENCH_VLLM_COMMON_CPU_MEM="0"

export LLMDBENCH_VLLM_MODELSERVICE_DECODE_ACCELERATOR_NR="0"
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_ACCELERATOR_NR="0"
export LLMDBENCH_VLLM_MODELSERVICE_URI_PROTOCOL="NA"
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_MODEL_COMMAND=imageDefault
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_MODEL_COMMAND=imageDefault
export LLMDBENCH_VLLM_MODELSERVICE_DECODE_EXTRA_ARGS="[]"
export LLMDBENCH_VLLM_MODELSERVICE_PREFILL_EXTRA_ARGS="[]"

export LLMDBENCH_HARNESS_CPU_NR="0"
export LLMDBENCH_HARNESS_CPU_MEM="0"
